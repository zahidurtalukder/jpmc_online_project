{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c5603c",
   "metadata": {},
   "source": [
    "# **Loan Default Prediction**\n",
    "\n",
    "This notebook demonstrates training a **PyTorch neural network** to predict whether a loan will default.\n",
    "\n",
    "## **Notebook Outline**\n",
    "1. **Train-Test Split & Imbalance Handling**\n",
    "2. **PyTorch Model Building & Training**\n",
    "3. **Evaluation & Threshold Tuning**\n",
    "\n",
    "We'll see both **Markdown explanations** (like this one) and **Code cells** demonstrating each step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "987dfdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-7137d4bfe437>:23: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')  # optional aesthetics\n"
     ]
    }
   ],
   "source": [
    "# (Cell 1) 1. Imports & Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (confusion_matrix, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, classification_report)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "plt.style.use('seaborn')  # optional aesthetics\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f1a0e",
   "metadata": {},
   "source": [
    "## **Cleaned Data Loading For Traning**\n",
    "We load the \"cleaned_train_data.csv\" file that we saved after EDA analysis for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3e261d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_train_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1c93c",
   "metadata": {},
   "source": [
    "## **6. Train-Test Split & Imbalance Handling**\n",
    "\n",
    "We'll do a stratified split (to preserve the ~7% minority class). Then we optionally oversample the minority class using `RandomOverSampler` from `imblearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4629d9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (151565, 20) Val shape: (37892, 20)\n",
      "Positive class in train: 0.06929040345726256\n",
      "Positive class in val:   0.06930222738308878\n",
      "After oversampling: (162222, 20)\n",
      "Positive class proportion: 0.13043237045530198\n"
     ]
    }
   ],
   "source": [
    "# (Cell 5) Train-Val Split, Scale, and Oversample\n",
    "\n",
    "y = df['bad_flag'].astype(int)\n",
    "X = df.drop(columns=['bad_flag'])\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Val shape:\", X_val.shape)\n",
    "print(\"Positive class in train:\", (y_train==1).mean())\n",
    "print(\"Positive class in val:  \", (y_val==1).mean())\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "# Convert back to DataFrame (optional)\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_val   = pd.DataFrame(X_val_scaled,   columns=X_val.columns,   index=X_val.index)\n",
    "\n",
    "# OverSampling\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy=0.15)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "print(\"After oversampling:\", X_train_ros.shape)\n",
    "print(\"Positive class proportion:\", (y_train_ros==1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda32aa",
   "metadata": {},
   "source": [
    "## **7. PyTorch Model Building & Training**\n",
    "We'll define a **DeeperNet** with a couple of hidden layers. We'll use a **pos_weight** in the BCE loss to handle imbalance. Then train for a set number of epochs.\n",
    "\n",
    "**Why this architecture?**\n",
    "- A 2-layer MLP is more expressive than a single-layer and still relatively fast.\n",
    "- We use ReLU activation for simplicity.\n",
    "- We use Adam optimizer with a small learning rate. In practice, you might tune more extensively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2b894e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeeperNet(\n",
      "  (fc1): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "[Epoch 1/50] Train Loss: 1.5671, Train Acc: 0.3131 | Val Loss: 1.3134, Val Acc: 0.2702\n",
      "[Epoch 2/50] Train Loss: 1.5524, Train Acc: 0.3358 | Val Loss: 1.3035, Val Acc: 0.3036\n",
      "[Epoch 3/50] Train Loss: 1.5464, Train Acc: 0.3386 | Val Loss: 1.3016, Val Acc: 0.3091\n",
      "[Epoch 4/50] Train Loss: 1.5410, Train Acc: 0.3428 | Val Loss: 1.2671, Val Acc: 0.3362\n",
      "[Epoch 5/50] Train Loss: 1.5351, Train Acc: 0.3479 | Val Loss: 1.3114, Val Acc: 0.2675\n",
      "[Epoch 6/50] Train Loss: 1.5313, Train Acc: 0.3488 | Val Loss: 1.2871, Val Acc: 0.3022\n",
      "[Epoch 7/50] Train Loss: 1.5266, Train Acc: 0.3533 | Val Loss: 1.3071, Val Acc: 0.2934\n",
      "[Epoch 8/50] Train Loss: 1.5204, Train Acc: 0.3573 | Val Loss: 1.2674, Val Acc: 0.3449\n",
      "[Epoch 9/50] Train Loss: 1.5170, Train Acc: 0.3597 | Val Loss: 1.2864, Val Acc: 0.3529\n",
      "[Epoch 10/50] Train Loss: 1.5130, Train Acc: 0.3656 | Val Loss: 1.2896, Val Acc: 0.3388\n",
      "[Epoch 11/50] Train Loss: 1.5079, Train Acc: 0.3700 | Val Loss: 1.3189, Val Acc: 0.3155\n",
      "[Epoch 12/50] Train Loss: 1.5044, Train Acc: 0.3742 | Val Loss: 1.3208, Val Acc: 0.3227\n",
      "[Epoch 13/50] Train Loss: 1.4989, Train Acc: 0.3793 | Val Loss: 1.3002, Val Acc: 0.3496\n",
      "[Epoch 14/50] Train Loss: 1.4944, Train Acc: 0.3836 | Val Loss: 1.2963, Val Acc: 0.3567\n",
      "[Epoch 15/50] Train Loss: 1.4884, Train Acc: 0.3917 | Val Loss: 1.3210, Val Acc: 0.3269\n",
      "[Epoch 16/50] Train Loss: 1.4832, Train Acc: 0.3957 | Val Loss: 1.3144, Val Acc: 0.3289\n",
      "[Epoch 17/50] Train Loss: 1.4782, Train Acc: 0.4002 | Val Loss: 1.3432, Val Acc: 0.3256\n",
      "[Epoch 18/50] Train Loss: 1.4745, Train Acc: 0.4034 | Val Loss: 1.2916, Val Acc: 0.3729\n",
      "[Epoch 19/50] Train Loss: 1.4696, Train Acc: 0.4084 | Val Loss: 1.3111, Val Acc: 0.3728\n",
      "[Epoch 20/50] Train Loss: 1.4639, Train Acc: 0.4110 | Val Loss: 1.3096, Val Acc: 0.3699\n",
      "[Epoch 21/50] Train Loss: 1.4594, Train Acc: 0.4182 | Val Loss: 1.3623, Val Acc: 0.3217\n",
      "[Epoch 22/50] Train Loss: 1.4537, Train Acc: 0.4187 | Val Loss: 1.3077, Val Acc: 0.4049\n",
      "[Epoch 23/50] Train Loss: 1.4490, Train Acc: 0.4238 | Val Loss: 1.3047, Val Acc: 0.3900\n",
      "[Epoch 24/50] Train Loss: 1.4449, Train Acc: 0.4286 | Val Loss: 1.3322, Val Acc: 0.3783\n",
      "[Epoch 25/50] Train Loss: 1.4408, Train Acc: 0.4325 | Val Loss: 1.3320, Val Acc: 0.3723\n",
      "[Epoch 26/50] Train Loss: 1.4372, Train Acc: 0.4338 | Val Loss: 1.3499, Val Acc: 0.3751\n",
      "[Epoch 27/50] Train Loss: 1.4330, Train Acc: 0.4391 | Val Loss: 1.3495, Val Acc: 0.3844\n",
      "[Epoch 28/50] Train Loss: 1.4298, Train Acc: 0.4420 | Val Loss: 1.3181, Val Acc: 0.4330\n",
      "[Epoch 29/50] Train Loss: 1.4264, Train Acc: 0.4453 | Val Loss: 1.3449, Val Acc: 0.3574\n",
      "[Epoch 30/50] Train Loss: 1.4227, Train Acc: 0.4488 | Val Loss: 1.3371, Val Acc: 0.4232\n",
      "[Epoch 31/50] Train Loss: 1.4182, Train Acc: 0.4510 | Val Loss: 1.3450, Val Acc: 0.4125\n",
      "[Epoch 32/50] Train Loss: 1.4163, Train Acc: 0.4519 | Val Loss: 1.3272, Val Acc: 0.4235\n",
      "[Epoch 33/50] Train Loss: 1.4124, Train Acc: 0.4567 | Val Loss: 1.3500, Val Acc: 0.3995\n",
      "[Epoch 34/50] Train Loss: 1.4077, Train Acc: 0.4612 | Val Loss: 1.3519, Val Acc: 0.4144\n",
      "[Epoch 35/50] Train Loss: 1.4052, Train Acc: 0.4648 | Val Loss: 1.3703, Val Acc: 0.3776\n",
      "[Epoch 36/50] Train Loss: 1.4036, Train Acc: 0.4630 | Val Loss: 1.3438, Val Acc: 0.4537\n",
      "[Epoch 37/50] Train Loss: 1.4004, Train Acc: 0.4678 | Val Loss: 1.3819, Val Acc: 0.4180\n",
      "[Epoch 38/50] Train Loss: 1.3966, Train Acc: 0.4692 | Val Loss: 1.3533, Val Acc: 0.4236\n",
      "[Epoch 39/50] Train Loss: 1.3935, Train Acc: 0.4730 | Val Loss: 1.3840, Val Acc: 0.3999\n",
      "[Epoch 40/50] Train Loss: 1.3920, Train Acc: 0.4727 | Val Loss: 1.3563, Val Acc: 0.4506\n",
      "[Epoch 41/50] Train Loss: 1.3870, Train Acc: 0.4790 | Val Loss: 1.3704, Val Acc: 0.4388\n",
      "[Epoch 42/50] Train Loss: 1.3862, Train Acc: 0.4773 | Val Loss: 1.3740, Val Acc: 0.4658\n",
      "[Epoch 43/50] Train Loss: 1.3836, Train Acc: 0.4821 | Val Loss: 1.3854, Val Acc: 0.3953\n",
      "[Epoch 44/50] Train Loss: 1.3785, Train Acc: 0.4827 | Val Loss: 1.3690, Val Acc: 0.4374\n",
      "[Epoch 45/50] Train Loss: 1.3791, Train Acc: 0.4851 | Val Loss: 1.3914, Val Acc: 0.4074\n",
      "[Epoch 46/50] Train Loss: 1.3754, Train Acc: 0.4870 | Val Loss: 1.3681, Val Acc: 0.4911\n",
      "[Epoch 47/50] Train Loss: 1.3724, Train Acc: 0.4901 | Val Loss: 1.3943, Val Acc: 0.4186\n",
      "[Epoch 48/50] Train Loss: 1.3720, Train Acc: 0.4900 | Val Loss: 1.3820, Val Acc: 0.4487\n",
      "[Epoch 49/50] Train Loss: 1.3692, Train Acc: 0.4913 | Val Loss: 1.3810, Val Acc: 0.4377\n",
      "[Epoch 50/50] Train Loss: 1.3668, Train Acc: 0.4932 | Val Loss: 1.4035, Val Acc: 0.4296\n"
     ]
    }
   ],
   "source": [
    "# (Cell 6) Build PyTorch model & Dataloaders\n",
    "\n",
    "# Prepare final train sets\n",
    "use_oversample = True  # set to False if you want to skip oversampling\n",
    "\n",
    "if use_oversample:\n",
    "    X_train_final, y_train_final = X_train_ros, y_train_ros\n",
    "else:\n",
    "    X_train_final, y_train_final = X_train, y_train\n",
    "\n",
    "X_train_t = torch.tensor(X_train_final.values, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train_final.values, dtype=torch.float32)\n",
    "\n",
    "X_val_t = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset   = TensorDataset(X_val_t,   y_val_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False)\n",
    "\n",
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # no sigmoid here for BCEWithLogitsLoss\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "model = DeeperNet(input_dim=X_train.shape[1], hidden_dim=64)\n",
    "print(model)\n",
    "\n",
    "# pos_weight for imbalance\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "pos_weight_val = torch.tensor([neg_count / pos_count], dtype=torch.float32)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_val)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute train accuracy\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).float()\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total   += batch_y.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:\n",
    "            val_outputs = model(val_X).squeeze()\n",
    "            v_loss = criterion(val_outputs, val_y)\n",
    "            val_loss += v_loss.item()\n",
    "\n",
    "            val_probs = torch.sigmoid(val_outputs)\n",
    "            val_preds = (val_probs > 0.5).float()\n",
    "            val_correct += (val_preds == val_y).sum().item()\n",
    "            val_total   += val_y.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29966fbc",
   "metadata": {},
   "source": [
    "## **8. Evaluation & Threshold Tuning**\n",
    "We'll compute **precision, recall, F1, and AUC** on the validation set. Then we can see if a threshold other than 0.5 yields better F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c7734af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1006, Recall: 0.6070, F1: 0.1725, AUC: 0.6340\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.60      0.73     35266\n",
      "     class 1       0.10      0.61      0.17      2626\n",
      "\n",
      "    accuracy                           0.60     37892\n",
      "   macro avg       0.53      0.60      0.45     37892\n",
      "weighted avg       0.89      0.60      0.69     37892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (Cell 7) Final Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_X, val_y in val_loader:\n",
    "        logits = model(val_X).squeeze()\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_preds.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "# Default threshold = 0.5\n",
    "pred_labels = [1 if p >= 0.65 else 0 for p in all_preds]\n",
    "\n",
    "prec = precision_score(all_labels, pred_labels)\n",
    "rec  = recall_score(all_labels, pred_labels)\n",
    "f1   = f1_score(all_labels, pred_labels)\n",
    "auc  = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, pred_labels, target_names=[\"class 0\", \"class 1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6449f8",
   "metadata": {},
   "source": [
    "### Threshold Tuning\n",
    "To find the best F1 threshold, we can systematically try thresholds from 0.0 to 1.0 in small increments. This is just a demonstration—pick a threshold that aligns with your business needs (precision vs. recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "051e7a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.72\n",
      "Best F1 at that threshold: 0.1781589321529843\n"
     ]
    }
   ],
   "source": [
    "# (Cell 8) Threshold Tuning\n",
    "best_t, best_f1 = 0, 0\n",
    "import numpy as np\n",
    "\n",
    "for t in np.arange(0.0, 1.01, 0.01):\n",
    "    temp_preds = [1 if p >= t else 0 for p in all_preds]\n",
    "    current_f1 = f1_score(all_labels, temp_preds)\n",
    "    if current_f1 > best_f1:\n",
    "        best_f1 = current_f1\n",
    "        best_t = t\n",
    "\n",
    "print(\"Best threshold:\", best_t)\n",
    "print(\"Best F1 at that threshold:\", best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f5889",
   "metadata": {},
   "source": [
    "## **8. Data Alignment and Scaling for Test Set**\n",
    "This section ensures that the test dataset is properly aligned with the features used during model training. Missing columns are added with default values, and any unnecessary columns are removed. After alignment, the test data is scaled using the same scaler fitted on the training data to maintain consistency in feature scaling. Then we do the inference to do the **loan default prediction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eecf0d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-aab07725946a>:4: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_df = pd.read_csv(\"testing_loan_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"testing_loan_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c804022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert 'term' from \"36 months\" → 36\n",
    "if 'term' in test_df.columns:\n",
    "    test_df['term'] = test_df['term'].str.replace(' months', '', regex=False)\n",
    "    test_df['term'] = pd.to_numeric(test_df['term'], errors='coerce')\n",
    "\n",
    "# 2. Convert percentage columns like 'int_rate' and 'revol_util'\n",
    "for col in ['int_rate', 'revol_util']:\n",
    "    if col in test_df.columns:\n",
    "        test_df[col] = test_df[col].astype(str).str.replace('%', '', regex=False)\n",
    "        test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
    "\n",
    "# 3. Convert 'emp_length' to numeric\n",
    "def convert_emp_length(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    x = str(x).lower()\n",
    "    if '10+' in x:\n",
    "        return 10\n",
    "    elif '< 1' in x:\n",
    "        return 0\n",
    "    match = re.search(r'(\\d+)', x)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "if 'emp_length' in test_df.columns:\n",
    "    test_df['emp_length'] = test_df['emp_length'].apply(convert_emp_length)\n",
    "\n",
    "# 4. Handle missing values\n",
    "test_df['mths_since_last_major_derog'].fillna(999, inplace=True)\n",
    "test_df['mths_since_recent_inq'].fillna(36, inplace=True)\n",
    "\n",
    "num_impute_cols = [\n",
    "    'tot_cur_bal', 'bc_util', 'percent_bc_gt_75', 'emp_length',\n",
    "    'tot_hi_cred_lim', 'total_bc_limit', 'revol_util', 'dti',\n",
    "    'inq_last_6mths', 'annual_inc', 'int_rate', 'term'\n",
    "]\n",
    "for col in num_impute_cols:\n",
    "    test_df[col].fillna(test_df[col].median(), inplace=True)\n",
    "\n",
    "# 5. One-hot encode categorical columns\n",
    "categorical_cols = ['home_ownership', 'purpose']\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b93f520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the columns from the training set\n",
    "trained_columns = X_train.columns.tolist()\n",
    "\n",
    "# Drop unnecessary columns in test data\n",
    "test_df_cleaned = test_df.drop(columns=['desc', 'id', 'member_id', 'application_approved_flag', 'bad_flag'], errors='ignore')\n",
    "\n",
    "# Add missing columns with default values (0 for one-hot encoded columns)\n",
    "for col in trained_columns:\n",
    "    if col not in test_df_cleaned.columns:\n",
    "        test_df_cleaned[col] = 0\n",
    "\n",
    "# Ensure test data columns are in the same order as training\n",
    "X_test = test_df_cleaned[trained_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f2ac285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    " # Ensure columns match training features\n",
    "X_test_scaled = scaler.transform(X_test)  # Scale the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38669f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_tensor).squeeze()\n",
    "    probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "threshold = 0.70\n",
    "pred_labels = (probs >= threshold).int().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61184d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "test_df['bad_flag'] = pred_labels\n",
    "test_df[['id', 'bad_flag']].to_csv(\"test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f26e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
